# 실습
# 피처임포턴스가 전체 중요도 하위 20 ~ 25%인 컬럼들을 제거하여 데이터셋을 재구성한 후
# 각 모델별로 돌려서 결과 도출
# 기존 모델결과와 비교
# feature = column = 열

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier     # RandomForestClassifier는 DecisionTreeClassifier의 앙상블 모델
from xgboost import XGBClassifier, XGBRegressor
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd
from icecream import ic

### feature_importances
# 분류

# 1. 데이터
datasets = load_breast_cancer()
x = datasets.data
y = datasets.target



x = pd.DataFrame(x)

ic(x.shape, y.shape)
'''
    x.shape: (569, 30)
    y.shape: (569,)
'''

# Feature Importances 낮은 컬럼 삭제
x = x.drop([2,4,6,8,9,10,11,14,15,16,17,18,19,24,25,26,28,29], axis=1)
ic(x.shape)

x = x.to_numpy()




x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True, random_state=66)


# 2. 모델
# model = DecisionTreeClassifier(max_depth=5)
# model = RandomForestClassifier()
# model = GradientBoostingClassifier()
model = XGBClassifier()

# 3. 훈련
model.fit(x_train, y_train)


# 4. 평가, 예측
acc = model.score(x_test, y_test)
print('acc :', acc)

print(model.feature_importances_)       # feature_importances(컬럼의 중요도)_ : tree 계열에서 제공되는 강력한 애


'''
* DecisionTreeClassifier
삭제전
acc : 0.8947368421052632
[0.         0.06054151 0.         0.         0.         0.
 0.00636533 0.02005078 0.         0.         0.01257413 0.
 0.         0.00716099 0.         0.         0.02291518 0.00442037
 0.004774   0.         0.         0.01642816 0.         0.72839202
 0.         0.         0.         0.11637753 0.         0.        ]
 삭제후
 acc : 0.8947368421052632
[0.06054151 0.02928051 0.02005078 0.01973513 0.         0.
 0.00442037 0.004774   0.01642816 0.72839202 0.11637753 0.        ]


* RandomForestClassifier
삭제전
acc : 0.9649122807017544
[0.04594489 0.01396862 0.03249729 0.01881504 0.00589572 0.01088367
 0.0474708  0.12498551 0.00281653 0.00315305 0.01074243 0.00392675
 0.02002314 0.0399178  0.00331371 0.00354595 0.00358579 0.00318945
 0.0027572  0.00562694 0.17345586 0.02005404 0.15872429 0.06739186
 0.01508678 0.01327798 0.02374231 0.11083588 0.00650967 0.00786106]
 삭제후
acc : 0.9649122807017544
[0.02652255 0.01952368 0.04023108 0.03004593 0.00841995 0.06911906
 0.11140834 0.01328507 0.00930228 0.03820229 0.11506378 0.01982783
 0.19652776 0.15015124 0.01826935 0.01065134 0.02061129 0.10283718]


 * GradientBoostingClassifier
 삭제전
acc : 0.9473684210526315
[7.93527081e-05 3.63306577e-02 2.94107562e-04 2.02140285e-03
 9.59040402e-04 4.99673144e-05 5.29625322e-04 1.30923163e-01
 2.86918997e-03 1.20900355e-03 4.03602195e-03 1.09908846e-05
 2.26102595e-03 1.78774598e-02 1.47445346e-03 4.69983132e-03
 1.10354998e-03 7.07941148e-04 7.75375472e-06 1.56017032e-03
 3.12640179e-01 4.13256961e-02 3.57722535e-02 2.73214766e-01
 3.35735537e-03 1.00979149e-04 1.35896374e-02 1.10922291e-01
 5.29557759e-05 1.91771648e-05]
 삭제후
 acc : 0.956140350877193
[3.92985837e-02 2.04965164e-03 1.30381503e-01 2.85981003e-03
 3.00627947e-04 4.02636743e-03 2.74359076e-03 1.80073790e-02
 1.46978707e-03 8.96104482e-03 2.43446827e-03 7.94285522e-03
 3.14552552e-01 3.88663147e-02 3.55628060e-02 2.72713271e-01
 4.20204937e-03 1.37661535e-02 9.98611847e-02]


 * XGBClassifier
 삭제전
acc : 0.9736842105263158
[0.01420499 0.03333857 0.         0.02365488 0.00513449 0.06629944
 0.0054994  0.09745205 0.00340272 0.00369179 0.00769184 0.00281184
 0.01171023 0.0136856  0.00430626 0.0058475  0.00037145 0.00326043
 0.00639412 0.0050556  0.01813928 0.02285903 0.22248562 0.28493083
 0.00233393 0.         0.00903706 0.11586285 0.00278498 0.00775311]
 삭제후
acc : 0.9824561403508771
[0.00553045 0.09065999 0.01021949 0.0141299  0.08260347 0.00853483
 0.01889274 0.02227781 0.03122359 0.18543425 0.45491385 0.07557958]
'''